{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <center> Hacker News Pipeline \n",
    "    \n",
    "The data we will use comes from a [Hacker News(HN)](https://news.ycombinator.com/) API that returns JSON data of the top stories in 2014. To make things easier, data have been already downloaded a list of JSON posts to a file called 'hn_stories_2014.json'. The goal will be to find the top 100 keywords of Hacker News Posts in 2014. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Pipeline module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import Pipeline \n",
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the JSON Data\n",
    "\n",
    "Because JSON files resemble a key-value dictionary, the goal is to parse the JSON file into a Python dict object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "@pipeline.task() \n",
    "def file_to_json() : \n",
    "    with open('hn_stories_2014.json', 'r') as f : \n",
    "        data = json.load(f)\n",
    "        stories = data['stories']\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the Stories \n",
    "\n",
    "We can filter for popular stories by ensuring they are links (not Ask HN posts), have a good number of points, and have some comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on = file_to_json) \n",
    "def filter_stories(stories) : \n",
    "    def check_popular(story) : \n",
    "        return story['points'] > 50 and story['num_comments'] > 1 and not story['title'].startswith('Ask HN')\n",
    "    return (story for story in stories if check_popular(story)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to CSV\n",
    "\n",
    "with a reduced set of stories, it's time to write these dict objects to a CSV file. The purpose of translating the dictionaries to a CSV is that we want to have a consistenet data format when running the last summarizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from pipeline import build_csv\n",
    "from datetime import datetime \n",
    "\n",
    "@pipeline.task(depends_on = filter_stories)\n",
    "def json_to_csv(stories) :\n",
    "    rows = [] \n",
    "    for story in stories : \n",
    "        rows.append((story['objectID'], datetime.strptime(story['created_at'], \"%Y-%m-%dT%H:%M:%SZ\"), \n",
    "                     story['url'], story['points'], story['title']))\n",
    "    return bulid_csv(rows, header = ['objecID', 'created_at', 'url', 'points', 'title'], file = io.StringIO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Title Columns\n",
    "\n",
    "Using the CSV file format we created, we can now extract the title column. Once we have extracted the title of each popular most, we can then run the next word frequency task. To extract the titles, we'll follow the steps following : \n",
    "\n",
    "1. Import csv, and create a csv.reader() object from the file object.\n",
    "2. Find the index of the title in the header.\n",
    "3. Iterate the through the reaer, and return eac hitem from the reader in the corresponding title index position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "@pipeline.task(depends_on = json_to_csv)\n",
    "def extract_titles(csv_file) : \n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    idx = header.index('title')\n",
    "    return (row[idx] for row in reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Titles \n",
    "\n",
    "Because we're trying to create a word frequency model of words from Hacker News titles, we need a way to create a consistent set of wrods to use. For example, words like Google, google, Google?, and google., all means the same keyword: gooogle. If we were to split hte title into words, however, they would all be lumped into different categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on = extract_titles)\n",
    "def clean_titles(titles) : \n",
    "    for title in titles : \n",
    "        title = title.lower() \n",
    "        title = ''.join(char for char in title if char not in string.punctuation)\n",
    "        yield title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Word Frequency Dictionary \n",
    "\n",
    "With a cleaned title, we can now build the word frequency dictionary. A word frequency dictionary are key value pairs that connects a word to the number of times it is used in a text. \n",
    "\n",
    "Furthermore, to find actual keywords, we should enforce the word frequency dictionary to not include stop words. Stop words are words that occur frequently in language like \"the\", \"or\", etc., and are commonly rejected in keyword searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on = clean_titles)\n",
    "def build_keyword_dictionary(cleaned_titles) : \n",
    "    words_freq = {} \n",
    "    for title in cleaned_titles : \n",
    "        for word in title.split(\" \"): \n",
    "            if word and word not in stop_words : \n",
    "                if word not in words_freq : \n",
    "                    words_freq[word] = 1\n",
    "                words_freq[word] += 1\n",
    "    return words_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the Top Words \n",
    "\n",
    "The toal is to output a list of tuples with (word, frequency) as the entries sorted from most used, to leas most used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on = build_keyword_dictionary)\n",
    "def top100_words(words_freq):\n",
    "    top100_freq = [(word, freq) for word, freq in sorted(words_freq.items(), key = lambda x: x[1], reverse = True)]\n",
    "    return words_freq[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('new', 186)\n",
      "('google', 168)\n",
      "('bitcoin', 102)\n",
      "('open', 93)\n",
      "('programming', 91)\n",
      "('web', 89)\n",
      "('data', 86)\n",
      "('video', 80)\n",
      "('python', 76)\n",
      "('code', 73)\n",
      "('facebook', 72)\n",
      "('released', 72)\n",
      "('using', 71)\n",
      "('2013', 66)\n",
      "('javascript', 66)\n",
      "('free', 65)\n",
      "('source', 65)\n",
      "('game', 64)\n",
      "('internet', 63)\n",
      "('microsoft', 60)\n",
      "('c', 60)\n",
      "('linux', 59)\n",
      "('app', 58)\n",
      "('pdf', 56)\n",
      "('work', 55)\n",
      "('language', 55)\n",
      "('software', 53)\n",
      "('2014', 53)\n",
      "('startup', 52)\n",
      "('apple', 51)\n",
      "('use', 51)\n",
      "('make', 51)\n",
      "('time', 49)\n",
      "('yc', 49)\n",
      "('security', 49)\n",
      "('nsa', 46)\n",
      "('github', 46)\n",
      "('windows', 45)\n",
      "('world', 42)\n",
      "('way', 42)\n",
      "('like', 42)\n",
      "('1', 41)\n",
      "('project', 41)\n",
      "('computer', 41)\n",
      "('heartbleed', 41)\n",
      "('git', 38)\n",
      "('users', 38)\n",
      "('dont', 38)\n",
      "('design', 38)\n",
      "('ios', 38)\n",
      "('developer', 37)\n",
      "('os', 37)\n",
      "('twitter', 37)\n",
      "('ceo', 37)\n",
      "('vs', 37)\n",
      "('life', 37)\n",
      "('big', 36)\n",
      "('day', 36)\n",
      "('android', 35)\n",
      "('online', 35)\n",
      "('years', 34)\n",
      "('simple', 34)\n",
      "('court', 34)\n",
      "('guide', 33)\n",
      "('learning', 33)\n",
      "('mt', 33)\n",
      "('api', 33)\n",
      "('says', 33)\n",
      "('apps', 33)\n",
      "('browser', 33)\n",
      "('server', 32)\n",
      "('firefox', 32)\n",
      "('fast', 32)\n",
      "('gox', 32)\n",
      "('problem', 32)\n",
      "('mozilla', 32)\n",
      "('engine', 32)\n",
      "('site', 32)\n",
      "('introducing', 31)\n",
      "('amazon', 31)\n",
      "('year', 31)\n",
      "('support', 30)\n",
      "('stop', 30)\n",
      "('built', 30)\n",
      "('better', 30)\n",
      "('million', 30)\n",
      "('people', 30)\n",
      "('text', 30)\n",
      "('3', 29)\n",
      "('does', 29)\n",
      "('tech', 29)\n",
      "('development', 29)\n",
      "('billion', 28)\n",
      "('developers', 28)\n",
      "('just', 28)\n",
      "('library', 28)\n",
      "('did', 28)\n",
      "('website', 28)\n",
      "('money', 28)\n",
      "('inside', 28)\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import csv \n",
    "import json\n",
    "import string \n",
    "from datetime import datetime \n",
    "\n",
    "from pipeline import build_csv, Pipeline \n",
    "from stop_words import stop_words \n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "@pipeline.task() \n",
    "def file_to_json() : \n",
    "    with open('hn_stories_2014.json', 'r') as f : \n",
    "        data = json.load(f)\n",
    "        stories = data['stories']\n",
    "    return stories\n",
    "\n",
    "@pipeline.task(depends_on = file_to_json) \n",
    "def filter_stories(stories) : \n",
    "    def check_popular(story) : \n",
    "        return story['points'] > 50 and story['num_comments'] > 1 and not story['title'].startswith('Ask HN')\n",
    "    return (story for story in stories if check_popular(story)) \n",
    "\n",
    "@pipeline.task(depends_on = filter_stories)\n",
    "def json_to_csv(stories) :\n",
    "    rows = [] \n",
    "    for story in stories : \n",
    "        rows.append((story['objectID'], datetime.strptime(story['created_at'], \"%Y-%m-%dT%H:%M:%SZ\"), story['url'], story['points'], story['title']))\n",
    "    return build_csv(rows, header = ['objecID', 'created_at', 'url', 'points', 'title'], file = io.StringIO())\n",
    "\n",
    "@pipeline.task(depends_on = json_to_csv)\n",
    "def extract_titles(csv_file) : \n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    idx = header.index('title')\n",
    "    return (row[idx] for row in reader)\n",
    "\n",
    "@pipeline.task(depends_on = extract_titles)\n",
    "def clean_titles(titles) : \n",
    "    for title in titles : \n",
    "        title = title.lower() \n",
    "        title = ''.join(char for char in title if char not in string.punctuation)\n",
    "        yield title\n",
    "    \n",
    "@pipeline.task(depends_on = clean_titles)\n",
    "def build_keyword_dictionary(cleaned_titles) : \n",
    "    words_freq = {} \n",
    "    for title in cleaned_titles : \n",
    "        for word in title.split(\" \"): \n",
    "            if word and word not in stop_words : \n",
    "                if word not in words_freq : \n",
    "                    words_freq[word] = 1\n",
    "                words_freq[word] += 1\n",
    "    return words_freq\n",
    "\n",
    "@pipeline.task(depends_on = build_keyword_dictionary)\n",
    "def top100_words(words_freq):\n",
    "    top100_freq = [(word, freq) for word, freq in sorted(words_freq.items(), key = lambda x: x[1], reverse = True)]\n",
    "    return top100_freq[:100]\n",
    "\n",
    "result = pipeline.run()\n",
    "for word in result[top100_words] : \n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
